%%
%%  WEB Specific options
%%
{http_header,	[
		{"User-Agent", "Mozilla/5.0 ebot/1"},
	      	{"Accept-Charset", "utf-8"},
	      	{"Accept", "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8"},
	      	{"Accept-Language", "en-us,en;q=0.5"}
]}.

{http_options,	[
		{autoredirect, true}, 
	    	{timeout, 10000}
]}.


{request_options, [
%		  {proxy, {{"proxy.mycompany.com", 80}, ["localhost"]}}
		  {proxy, noproxy}
]}.


%%
%%  WEB CRAWLERS Specific options
%%

%% options: 
%%    add_final_slash
%%    to_lower_case : urls are case insensive and some web pages have links with some uppercase letters..
%%    without_internal_links
%%    without_queries,
%%    {max_depth, 2}
%%          the url path will be truncated to a max_depth path 
%%          http://www.redaelli.org/matteo/blog/a/ ->  http://www.redaelli.org/matteo/blog/
%%          should be the same as "tot_new_urls_queues" in ebot_amqp.conf
%%	    you should also start at least one crawler for depth in [0,max_depth]. see "crawler_pools" in this file
%%    (TODO)  {remove_filename, false}
{normalize_url, [
		{replace_string, [
    		%% http://www.gettyre.it/motoweb/XXX;jsessionid=250485C.sae_1
	      	   {";[A-Za-z0-9]+=[^&;?]+", ""},
    		%% some sites have newlines in url links: 
		%% see http://opensource.linux-mirror.org/index.php
    		%% TODO maybe it still doesn t work
	      	   {"\n",""},
    		%% http://github.com/dizzyd/ibrowse
	      	   {"&quot$",""}
		]},
		add_final_slash,
		{max_depth, 2}, 
		to_lower_case,
		without_internal_links,
		without_queries
		]}.

%% at least one regexp must be defined

{mime_any_regexps, [
	{match,   "^text/"}
%	{nomatch, "^image/"},
%	{nomatch, "^application/"}
	]
}.

{url_all_regexps, [
        {nomatch, "/bugs/"},
%% Skipping Apache.org urls
        {nomatch, "\\.apache\\..+/dist/"},
	{nomatch, "/snapshots/"},
	{nomatch, "bugs.+/issue"},
%% Skipping unwanted files
	{nomatch, "\\.deb$"},
	{nomatch, "\\.git$"},
	{nomatch, "\\.tgz"},
	{nomatch, "\\.jar$"},
	{nomatch, "\\.rpm$"},
	{nomatch, "\\.tar\\.gz"},

%% Skipping Github unseful pages
    	{nomatch, "github\\.+/issues"},
	{nomatch, "gist\\.github\\.com"},
	%% the page gives incomplete header
	{nomatch, "svn\\.github\\.com"},
%% Skipping Gitorious unseful pages
	{nomatch, "git.+/merge_requests/"},
	{nomatch, "git.+/commits/"},
	{nomatch, "git.+/trees/"},
%% Skipping Git repositories	  		
	{nomatch, "git.+/commit/"},
	{nomatch, "git.+/tree/"},
%% Skipping HG repositories
 	{nomatch, "/changeset/"},
%% Skipping SVN repositories
	{nomatch, "svn.+/viewvc/.+/"},
	{nomatch, "http://svn\\."},
   	{nomatch, "/branches"},
	{nomatch, "/trunk"},
	{nomatch, "/tags"}
	]}.

%% at least one regexp must be defined

{url_any_regexps, [
%	{match,"."}    
  	{match, "freshmeat\\.net"},
  	{match, "github\\.com"},
  	{match, "code\\.google\\.com"},
  	{match, "sourceforge\\.net"},
	{match, "ohloh\\.net"},
	{match, "bitbucket\\.org"},
%	{nomatch, "\\.com"},
	{match, "\\.org"},
	{match, "\\.net"}
	]
}.

%% after how many days, an ur that is stored in the DB will become obsolete

{obsolete_urls_after_days, 1}.

%% how many crawler threads will be started for each candidated url queue/depth
%% {crawler_pools, [{0,3},{1,2},{2,1}]} means
%% 3 crawlers will analyze urls got from AMQP queue ebot.new.0 that countains urls with depth==0 
%%   (ex. http://www.redaelli.org, http://www.redaelli.org/index.html)
%% 2 crawlers will analyze urls got from AMQP queue ebot.new.1 that countains urls with depth==1
%%   (ex. http://www.redaelli.org/matteo/, http://www.redaelli.org/matteo/index.html)
%% 1 crawlers will analyze urls got from AMQP queue ebot.new.2 that countains urls with depth==2

{crawler_pools, [{0,4}, {1,2}, {2,1}] }.

%% with true, the crawlers are restarted automtically after a crash of the node 
{start_crawlers_at_boot, true}.

