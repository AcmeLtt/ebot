%% -*- mode: erlang -*-
%%
%%  WEB Specific options
%%
{http_header,	[
		{"User-Agent", "Mozilla/5.0 ebot/0.3"},
	      	{"Accept-Charset", "utf-8"},
	      	{"Accept", "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8"},
	      	{"Accept-Language", "en-us,en;q=0.5"}
]}.

{http_options,	[
		{autoredirect, true}, 
	    	{timeout, 10000}
]}.


{request_options, [
%		  {proxy, {{"proxy.mycompany.com", 80}, ["localhost"]}}
		  {proxy, noproxy}
]}.


%%
%%  WEB CRAWLERS Specific options
%%

%% -------------------------------------------------------------------------------------------------
%% normalize_url
%% -------------------------------------------------------------------------------------------------
%%
%% options: 
%%    add_final_slash
%%    to_lower_case : urls are case insensive and some web pages have links with some uppercase letters..
%%    without_internal_links
%%    without_queries,
%%    {max_depth, 2}
%%          the url path will be truncated to a max_depth path 
%%          http://www.redaelli.org/matteo/blog/a/ ->  http://www.redaelli.org/matteo/blog/
%%          should be the same as "tot_new_urls_queues" in ebot_mq.conf
%%	    you should also start at least one crawler for depth in [0,max_depth]. see "crawler_pools" in this file
%%    (TODO)  {remove_filename, false}
{normalize_url, [
		{replace_string, [
    		%% http://www.gettyre.it/motoweb/XXX;jsessionid=250485C.sae_1
	      	   {";[A-Za-z0-9]+=[^&;?]+", ""},
    		%% some sites have newlines in url links: 
		%% see http://opensource.linux-mirror.org/index.php
    		%% TODO maybe it still doesn t work
	      	   {"\n",""},
    		%% http://github.com/dizzyd/ibrowse
	      	   {"&quot$",""}
		]},
		add_final_slash,
		{max_depth, 2}, 
		to_lower_case,
		without_internal_links,
		without_queries
		]}.

%% -------------------------------------------------------------------------------------------------
%% tobe_saved_headers
%% -------------------------------------------------------------------------------------------------
%% headers (if exist) that will be saved in the database
{tobe_saved_headers, 
    [
     <<"content-length">>,
     <<"content-type">>,
%    <<"date">>,
%    <<"last-modified">>,
     <<"server">>,
     <<"x-powered-by">>
    ]}.


%% -------------------------------------------------------------------------------------------------
%% mime_any_regexp
%% -------------------------------------------------------------------------------------------------
%%
%% the url will be analyzed if at least one regexp will be satisfied
%%
%% at least one regexp must be defined

{mime_any_regexps, [
	{match,   "^text/"}
%	{nomatch, "^image/"},
%	{nomatch, "^application/"}
	]
}.

%% -------------------------------------------------------------------------------------------------
%% url_all_regexps
%% -------------------------------------------------------------------------------------------------
%%
%% the url will be analyzed if ALL regexps will be satisfied
%%
{url_all_regexps, [
        {nomatch, "^https"},
        {nomatch, "//.+//"},
        {nomatch, "/bugs/"},
	{nomatch, "viewcvs\\.cgi"},

%% Skipping Apache.org urls
        {nomatch, "\\.apache\\..+/dist/"},
	{nomatch, "/snapshots/"},
        {nomatch, "^http://mail-archives"},
	{nomatch, "bugs.+/.+"},
	%% apache mirror sites.. TODO
	{nomatch, "apache\\.fastbull\\.org/.+"},

%% Skipping unwanted files
	{nomatch, "\\.deb$"},
	{nomatch, "\\.git$"},
	{nomatch, "\\.tgz"},
	{nomatch, "\\.jar$"},
	{nomatch, "\\.rpm$"},
	{nomatch, "\\.tar\\.gz"},

% Skipping CVS repositories
	{nomatch, "/cvs/\\."},

%% Skipping Github unseful pages
    	{nomatch, "github\\.+/issues"},
	{nomatch, "gist\\.github\\.com"},
	%% the page gives incomplete header
	{nomatch, "svn\\.github\\.com"},

%% Skipping Gitorious unseful pages
	{nomatch, "git.+/merge_requests/"},
	{nomatch, "git.+/commits/"},
	{nomatch, "git.+/trees/"},

%% Skipping Git repositories	  		
	{nomatch, "git.+/commit/"},
	{nomatch, "git.+/tree/"},

%% Skipping HG repositories
 	{nomatch, "/changeset/"},

%% Skipping SVN repositories
	{nomatch, "svn.+/viewvc/.+/"},
	{nomatch, "http://svn\\."},
   	{nomatch, "/branches"},
	{nomatch, "/trunk"},
	{nomatch, "/tags"}
	]}.

%% -------------------------------------------------------------------------------------------------
%% url_any_regexp
%% -------------------------------------------------------------------------------------------------
%%
%% the url will be analyzed if at least one regexp will be satisfied
%% at least one regexp must be defined

{url_any_regexps, [
%	{match,"."}    
  	{match, "freshmeat\\.net"},
  	{match, "github\\.com"},
  	{match, "code\\.google\\.com"},
  	{match, "sourceforge\\.net"},
	{match, "ohloh\\.net"},
	{match, "bitbucket\\.org"},
%	{nomatch, "\\.com"},
	{match, "\\.org"},
	{match, "\\.net"}
	]
}.

%% -------------------------------------------------------------------------------------------------
%% obsolete_urls_after_day
%% -------------------------------------------------------------------------------------------------
%%
%% after how many days, an ur that is stored in the DB will become obsolete

{obsolete_urls_after_days, 1}.

%% -------------------------------------------------------------------------------------------------
%% save_referrals
%% -------------------------------------------------------------------------------------------------
%%
%% values: external, internal

{save_referrals, [external]}.

%% -------------------------------------------------------------------------------------------------
%% send_body_to_mq
%% -------------------------------------------------------------------------------------------------
%%
%% if you need to run custom functions over the body of visited urls
%% you can set this parameter to true, and then create a AMQP consumer
%% that can read the {url, body} from the processed queue, analyze then and then (eventually) update 
%% the db with the new custom info 

{send_body_to_mq, false}.

%% -------------------------------------------------------------------------------------------------
%% crawlers_pool
%% -------------------------------------------------------------------------------------------------
%%
%% how many crawler threads will be started for each candidated url queue/depth
%% {crawler_pools, [{0,3},{1,2},{2,1}]} means
%% 3 crawlers will analyze urls got from AMQP queue ebot.new.0 that countains urls with depth==0 
%%   (ex. http://www.redaelli.org, http://www.redaelli.org/index.html)
%% 2 crawlers will analyze urls got from AMQP queue ebot.new.1 that countains urls with depth==1
%%   (ex. http://www.redaelli.org/matteo/, http://www.redaelli.org/matteo/index.html)
%% 1 crawlers will analyze urls got from AMQP queue ebot.new.2 that countains urls with depth==2

{crawler_pools, [{0,4}, {1,2}, {2,1}] }.

%% -------------------------------------------------------------------------------------------------
%% start_crawlers_at_boot
%% -------------------------------------------------------------------------------------------------
%%
%% are the crawlers started automatically at boot time? 
{start_crawlers_at_boot, false}.

%% -------------------------------------------------------------------------------------------------
%% crawlers_sleep_time
%% -------------------------------------------------------------------------------------------------
%%
%% how many milliseconds will each crawler sleep between two url crawls?
%% this option is useful in order to avoid heavy workloads to teh visited sites
%% and for the ebot system if the hardware is not powerful enough

{crawlers_sleep_time, 10000}.

