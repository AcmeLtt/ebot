%%
%%  WEB Specific options
%%
{http_header,	[
		{"User-Agent", "Mozilla/5.0 ebot/1"},
	      	{"Accept-Charset", "utf-8"},
	      	{"Accept", "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8"},
	      	{"Accept-Language", "en-us,en;q=0.5"}
]}.

{http_options,	[
		{autoredirect, true}, 
	    	{timeout, 60000}
]}.


{request_options, [
%		  {proxy, {{"proxy.mycompany.com", 80}, ["localhost"]}}
		  {proxy, noproxy}
]}.


%%
%%  WEB CRAWLERS Specific options
%%

%% options: 
%%    add_final_slash
%%    to_lower_case : urls are case insensive and some web pages have links with some uppercase letters..
%%    without_internal_links
%%    without_queries,
%%    {max_depth, 2}
%%          should be the same as ebot_amqp.conf
%%	    the url path will be truncated to a max_depth path 
%%    (TODO)  {remove_filename, false}
{normalize_url, [
		add_final_slash,
		{max_depth, 2}, 
		to_lower_case,
		without_internal_links,
		without_queries
		]}.

%% at least one regexp must be defined

{mime_any_regexps, [
	{match,   "^text/"}
%	{nomatch, "^image/"},
%	{nomatch, "^application/"}
	]
}.

{url_all_regexps, [
%% Skipping Apache.org urls
        {nomatch, "\.apache\..+/dist/"},
	{nomatch, "/snapshots/"},
	{nomatch, "bugs.+/issue"},
%% Skipping unwanted files
	{nomatch, "\\.deb$"},	
	{nomatch, "\\.rpm$"},
%% Skipping Github unseful pages
    	{nomatch, "github\.+/issues"},
	{nomatch, "gist\\.github\\.com"},
%% Skipping Gitorious unseful pages
	{nomatch, "git.+/merge_requests/"},
	{nomatch, "git.+/commits/"},
	{nomatch, "git.+/trees/"},
%% Skipping Git repositories	  		
	{nomatch, "git.+/commit/"},
	{nomatch, "git.+/tree/"},
%% Skipping HG repositories
 	{nomatch, "/changeset/"},
%% Skipping SVN repositories
	{nomatch, "svn.+/viewvc/.+/"},
	{nomatch, "svn\\.apache\\.org/.+/"},
   	{nomatch, "/branches/"},
	{nomatch, "/trunk/"},
	{nomatch, "/tags/"}
	]}.

%% at least one regexp must be defined

{url_any_regexps, [
%	{match,"."}    
  	{match, "freshmeat\\.net"},
  	{match, "github\\.com"},
  	{match, "code\\.google\\.com"},
  	{match, "sourceforge\\.net"},
	{match, "ohloh\\.net"},
	{match, "bitbucket\\.org"},
%	{nomatch, "\\.com"},
	{match, "\\.org"}
	]
}.


%% how many crawler threads will be started for each candidated url queue/depth
%% {crawler_pools, [{0,3},{1,2},{2,1}]}.

{crawler_pools, [{0,2}, {1,1}] }.	

